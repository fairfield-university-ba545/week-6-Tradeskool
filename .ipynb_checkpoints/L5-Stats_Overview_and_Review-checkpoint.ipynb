{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamental Stats Required for Data Science and Machine Learning in Python\n",
    "\n",
    "f you have little experience in applying machine learning algorithm, you would have discovered that it does not require any knowledge of Statistics as a prerequisite.\n",
    "\n",
    "However, knowing some statistics can be beneficial to understand machine learning technically as well intuitively. Knowing some statistics will eventually be required when you want to start validating your results and interpreting them. After all, when there is data, there are statistics. Like Mathematics is the language of Science. Statistics is one of a kind language for Data Science and Machine Learning.\n",
    "\n",
    "Statistics is a field of mathematics with lots of theories and findings. However, there are various concepts, tools, techniques, and notations are taken from this field to make machine learning what it is today. You can use descriptive statistical methods to help transform observations into useful information that you will be able to understand and share with others. You can use inferential statistical techniques to reason from small samples of data to whole domains. Later in this post, you will study descriptive and inferential statistics. So, don't worry.\n",
    "\n",
    "Before getting started, let's walk through ten examples where statistical methods are used in an applied machine learning project:\n",
    "\n",
    "- Problem Framing: Requires the use of exploratory data analysis and data mining.\n",
    "- Data Understanding: Requires the use of summary statistics and data visualization.\n",
    "- Data Cleaning: Requires the use of outlier detection, imputation and more.\n",
    "- Data Selection: Requires the use of data sampling and feature selection methods.\n",
    "- Data Preparation: Requires the use of data transforms, scaling, encoding and much more.\n",
    "- Model Evaluation: Requires experimental design and resampling methods.\n",
    "- Model Configuration: Requires the use of statistical hypothesis tests and estimation statistics.\n",
    "- Model Selection: Requires the use of statistical hypothesis tests and estimation statistics.\n",
    "- Model Presentation: Requires the use of estimation statistics such as confidence intervals.\n",
    "- Model Predictions: Requires the use of estimation statistics such as prediction intervals.\n",
    "\n",
    "Source: [Statistical Methods for Machine Learning](https://machinelearningmastery.com/statistics_for_machine_learning/)\n",
    "\n",
    "Isn't that fascinating?\n",
    "\n",
    "This post will give you a solid background in the essential but necessary statistics required for becoming a good machine learning practitioner.\n",
    "\n",
    "In this post, you will study:\n",
    "\n",
    "- Introduction to Statistics and its types\n",
    "- Statistics for data preparation\n",
    "- Statistics for model evaluation\n",
    "- Gaussian and Descriptive stats\n",
    "- Variable correlation\n",
    "- Non-parametric Statistics\n",
    "\n",
    "You have a lot to cover, and all of the topics are equally important. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Statistics and its types:\n",
    "Let's briefly study how to define statistics in simple terms.\n",
    "\n",
    "Statistics is considered a subfield of mathematics. It refers to a multitude of methods for working with data and using that data to answer many types of questions.\n",
    "\n",
    "When it comes to the statistical tools that are used in practice, it can be helpful to divide the field of statistics into two broad groups of methods: descriptive statistics for summarizing data, and inferential statistics for concluding samples of data ([Statistics for Machine Learning](https://machinelearningmastery.com/statistics_for_machine_learning/)).\n",
    "\n",
    "- Descriptive Statistics: Descriptive statistics are used to describe the essential features of the data in a study. They provide simple summaries about the sample and the measures. Together with simple graphics analysis, they form the basis of virtually every quantitative analysis of data. The below infographic provides a good summary of descriptive statistics:\n",
    "\n",
    "<img src = 'https://i2.wp.com/intellspot.com/wp-content/uploads/2017/11/descriptive-statistic-spreadsheet-and-pie-chart.png?resize=720%2C437' />\n",
    "_Source: IntellSpot_\n",
    "\n",
    "- Inferential Statistics: Inferential statistics are methods that help in quantifying properties of the domain or population from a tinier set of obtained observations called a sample. Below is an infographic which beautifully describes inferential statistics:\n",
    "\n",
    "<img src = 'https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/01/20150849/what-is-inferential-statistics.jpg' />\n",
    "_Source: Analytics Vidhya_\n",
    "\n",
    "In the next section, you will study the use of statistics for data preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics for data preparation:\n",
    "Statistical methods are required in the development of train and test data for your machine learning model.\n",
    "\n",
    "This includes techniques for:\n",
    "\n",
    "- Outlier detection\n",
    "- Missing value imputation\n",
    "- Data sampling\n",
    "- Data scaling\n",
    "- Variable encoding\n",
    "\n",
    "A basic understanding of data distributions, descriptive statistics, and data visualization is required to help you identify the methods to choose when performing these tasks.\n",
    "\n",
    "Let's analyze each of the above points briefly.\n",
    "\n",
    "### Outlier detection:\n",
    "Let's first see what an outlier is.\n",
    "\n",
    "An outlier is considered an observation that appears to deviate from other observations in the sample. The following figure makes the definition more prominent.\n",
    "\n",
    "<img src = 'https://www.mathworks.com/matlabcentral/mlc-downloads/downloads/submissions/34795/versions/7/screenshot.png' />\n",
    "_Source: MathWorks_\n",
    "\n",
    "You can spot the outliers (in __RED__) in the data as given the above figure.\n",
    "\n",
    "Many machine learning algorithms are sensitive to the range and distribution of attribute values in the input data. Outliers in input data can **skew** and **mislead** the training process of machine learning algorithms resulting in longer training times, less accurate models and ultimately more mediocre results.\n",
    "\n",
    "Identification of potential outliers is vital for the following reasons:\n",
    "\n",
    "- An outlier could indicate the data is bad. In example, the data maybe coded incorrectly, or the experiment did not run correctly. If it can be determined that an outlying point is, in fact, erroneous, then the value that is outlying should be removed from the analysis. If it is possible to correct that is another option.\n",
    "\n",
    "- In a few cases, it may not be possible to determine whether an outlying point is a bad data point. Outliers could be due to random variation or could possibly indicate something scientifically interesting. In any event, you typically do not want to just delete the outlying observation. However, if the data contains significant outliers, you may need to consider the use of robust statistical techniques.\n",
    "\n",
    "So, outliers are often not good for your predictive models (Although, sometimes, these outliers can be used as an advantage. But that is out of the scope of this post). You need the statistical know-how to handle outliers efficiently.\n",
    "\n",
    "### Missing value imputation:\n",
    "Well, most of the datasets now suffer from the problem of missing values. Your machine learning model may not get trained effectively if the data that you are feeding to the model contains missing values. Statistical tools and techniques come here for the rescue.\n",
    "\n",
    "Many people tend to discard the data instances which contain a missing value. But that is __NOT__ a good practice because during that course you may lose essential features/representations of the data. Although there are advanced methods for dealing with missing value problems, these are the quick techniques that one would go for: **Mean Imputation** and **Median Imputation**.\n",
    "\n",
    "It is imperative that you understand what mean and median are.\n",
    "\n",
    "Say, you have a feature X1 which has these values - `13, 18, 13, 14, 13, 16, 14, 21, 13`\n",
    "\n",
    "The mean is the usual average, so I'll add and then divide:\n",
    "\n",
    "`(13 + 18 + 13 + 14 + 13 + 16 + 14 + 21 + 13) / 9 = 15`\n",
    "\n",
    "Note that the mean, in this case, isn't a value from the original list. This is a common result. You should not assume that your mean will be one of your original numbers.\n",
    "\n",
    "The median is the middle value, so first, you will have to rewrite the list in numerical order:\n",
    "\n",
    "`13, 13, 13, 13, 14, 14, 16, 18, 21`\n",
    "\n",
    "There are nine numbers in the list, so the middle one will be the `(9 + 1) / 2 = 10 / 2 = 5th` number:\n",
    "\n",
    "`13, 13, 13, 13, 14, 14, 16, 18, 21`\n",
    "\n",
    "So the median is `14`.\n",
    "\n",
    "### Data sampling:\n",
    "Data is considered the currency of applied machine learning. Therefore, its collection and usage both are equally significant.\n",
    "\n",
    "Data sampling refers to statistical methods for selecting observations from the domain with the objective of estimating a population parameter. In other words, sampling is an active process of gathering observations with the intent of estimating a population variable.\n",
    "\n",
    "Each row of a dataset represents an observation that is indicative of a particular population. When working with data, you often do not have access to all possible observations. This could be for many reasons, for example:\n",
    "\n",
    "- It may be difficult or expensive to make more observations.\n",
    "- It may be challenging to gather all the observations together.\n",
    "- More observations are expected to be made in the future.\n",
    "\n",
    "__NOTE__: in data science/machine learning, we sample/split data since we need to validate model accuracy/efficiency - this is a __MUST-HAVE__.\n",
    "\n",
    "Many times, you will not have the right proportion of the data samples. So, you will have to *under-sample* or *over-sample* based on the type of problem.\n",
    "\n",
    "You perform under-sampling when the data samples for a particular category are very high compared to other meaning you discard some of the data samples from the category where they are higher. You perform over-sampling when the data samples for a particular type are decidedly lower compared to the other. In this case, you generate data samples.\n",
    "\n",
    "This applies to multi-class scenarios as well.\n",
    "\n",
    "Statistical sampling is a large field of study, but in applied machine learning, there may be three types of sampling that you are likely to use: simple random sampling, systematic sampling, and stratified sampling.\n",
    "\n",
    "- Simple Random Sampling: Samples are drawn with a uniform probability from the domain.\n",
    "- Systematic Sampling: Samples are drawn using a pre-specified pattern, such as at intervals.\n",
    "- Stratified Sampling: Samples are drawn within pre-specified categories (i.e., strata).\n",
    "\n",
    "Although these are the more common types of sampling that you may encounter, there are other techniques ([A Gentle Introduction to Statistical Sampling and Resampling](https://machinelearningmastery.com/statistical-sampling-and-resampling/)).\n",
    "\n",
    "### Data Scaling:\n",
    "Often, the features of your dataset may widely vary in ranges. Some features may have a scale of `0` to `100` while the other may have ranges of `0.01 - 0.001`, `10000- 20000`, etc.\n",
    "\n",
    "This is very problematic for efficient modeling. Because a small change in the feature which has a lower value range than the other feature may not have a significant impact on those other features. It affects the process of good learning. Dealing with this problem is known as **data scaling**.\n",
    "\n",
    "__NOTE__: you do __NOT__ want your features (continuous ones) at difference scales since the difference scales may affect your model differently.\n",
    "\n",
    "There are different data scaling techniques such as `Min-Max scaling`, `Absolute scaling`, `Standard scaling`, etc.\n",
    "\n",
    "### Variable encoding:\n",
    "\n",
    "At times, your datasets contain a mixture of both numeric and non-numeric data. Many machine learning frameworks like `scikit-learn` expect all the data to be present in all numeric format. This is also helpful to speed up the computation process.\n",
    "\n",
    "Again, statistics come for saving you.\n",
    "\n",
    "Techniques like `Label encoding, One-Hot encoding`, etc. are used to convert non-numeric data to numeric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's time to apply the techniques!\n",
    "You have covered a lot of theory for now. You will apply some of these to get the real feel.\n",
    "\n",
    "You will start off by applying some statistical methods to detect outliers.\n",
    "\n",
    "You will use the `Z-Score` index to detect outliers, and for this, you will investigate the [Boston House Price dataset](https://www.kaggle.com/c/boston-housing). Let's start off by importing the dataset from sklearn's utilities, and as you go along, you will start the necessary concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Load the Boston dataset into a variable called boston\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features from the target\n",
    "x = boston.data\n",
    "y = boston.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the dataset in a standard tabular format with the all the feature names, you will convert this into a `pandas` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the columns separately in a variable\n",
    "columns = boston.feature_names\n",
    "\n",
    "# Create the dataframe\n",
    "boston_df = pd.DataFrame(boston.data)\n",
    "boston_df.columns = columns\n",
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a common practice to start with univariate outlier analysis where you consider just one feature at a time. Often, a simple **box-plot** of a particular feature can give you good starting point. You will make a box-plot using `seaborn` and you will use the `DIS` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.boxplot(x=boston_df['DIS'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the box-plot, you import `matplotlib` since seaborn plots are displayed like ordinary `matplotlib` plots.\n",
    "\n",
    "The above plot shows three points between `10` to `12`, these are outliers as they're are not included in the box of other observations. Here you analyzed univariate outlier, i.e., you used DIS feature only to check for the outliers.\n",
    "\n",
    "Let's proceed with `Z-Score` now.\n",
    "\n",
    "\"*The Z-score is the signed number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured.*\" - [Wikipedia](https://en.wikipedia.org/?title=Z-score&redirect=no)\n",
    "\n",
    "The idea behind `Z-score` is to describe any data point regarding their relationship with the Standard Deviation and Mean for the group of data points. Z-score is about finding the distribution of data where the mean is `0`, and the standard deviation is `1`, i.e., __normal distribution__.\n",
    "\n",
    "Wait! How on earth does this help in identifying the outliers?\n",
    "\n",
    "Well, while calculating the Z-score you re-scale and center the data (mean of `0` and standard deviation of `1`) and look for the instances which are too far from zero. These data points that are way too far from zero are treated as the outliers. In most common cases the threshold of `3` or `-3` is used. In example, say the Z-score value is greater than or less than `3` or `-3` respectively. This data point will then be identified as an outlier.\n",
    "\n",
    "You will use the `Z-score` function defined in `scipy` library to detect the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "z = np.abs(stats.zscore(boston_df))\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not possible to detect the outliers by just looking at the above output. You are more intelligent! You will define the threshold for yourself, and you will use a simple condition for detecting the outliers that cross your threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = 3 # your defined threshold\n",
    "print(np.where(z > thres))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, a confusing output! The first array contains the list of row numbers and the second array contains their respective column numbers. For example, `z[55][1]` have a Z-score higher than `3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z[55][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the `54th` record on column `ZN` is an **outlier**. You can extend things from here.\n",
    "\n",
    "You saw how you could use Z-Score and set its threshold to detect potential outliers in the data. Next, you will see how to do some missing value imputation.\n",
    "\n",
    "You will use the famous [Pima Indian Diabetes dataset](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv) which is known to have missing values. But before proceeding any further, you will have to load the dataset into your workspace.\n",
    "\n",
    "You will load the dataset into a DataFrame object data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\",header=None)\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have already noticed that the column names are numeric here. This is because you are using an already preprocessed dataset. But don't worry, you will discover the names soon.\n",
    "\n",
    "Now, this dataset is known to have missing values, but for your first glance at the above statistics, it might appear that the dataset does not contain missing values at all. But if you take a closer look, you will find that there are some columns where a **zero** value is entirely **invalid**. These are the values that are missing.\n",
    "\n",
    "__NOTE__: Do __NOT__ expect missing values to be all NaNs.\n",
    "\n",
    "Specifically, the below columns have an invalid zero value as the minimum:\n",
    "\n",
    "- Plasma glucose concentration\n",
    "- Diastolic blood pressure\n",
    "- Triceps skinfold thickness\n",
    "- 2-Hour serum insulin\n",
    "- Body mass index\n",
    "\n",
    "Let's confirm this by looking at the raw data, the example prints the first `20` rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly there are `0` values in the columns `2`, `3`, `4`, and `5`.\n",
    "\n",
    "As this dataset has missing values denoted as `0`, so it might be tricky to handle it by just using the conventional means. Let's summarize the approach you will follow to combat this:\n",
    "\n",
    "- Get the count of zeros in each of the columns you saw earlier.\n",
    "- Determine which columns have the most zero values from the previous step.\n",
    "- Replace the zero values in those columns with `NaN`.\n",
    "- Check if the `NaN`s are getting appropriately reflected.\n",
    "- Call the `fillna()` function with the imputation strategy.\n",
    "\n",
    "__NOTE#1__: This is typically the standard procedure of handling missing values.\n",
    "\n",
    "__NOTE#2__: Sometimes you may face missing values in difference appearances (`0` or `-1` or `NaN`, etc.), it is __YOUR__ job to spot them!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the count of zeros in each of the columns\n",
    "print((data[[1,2,3,4,5]] == 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that columns `1`,`2` and `5` have just a few zero values, whereas columns `3` and `4` show a lot more, nearly half of the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Mark zero values as missing or NaN\n",
    "data[[1,2,3,4,5]] = data[[1,2,3,4,5]].replace(0, np.NaN)\n",
    "\n",
    "# Count the number of NaN values in each column\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compare the results and the results of above step to check if all `0`s are replaced to `NaN`s. Moreover, let's get sure at this point of time that your `NaN` replacement was a hit by taking a look at the dataset as a whole:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - you can compare the df below with the original data\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that marking the missing values had the intended effect.\n",
    "\n",
    "Up till now, you analyzed essential trends when data is missing and how you can make use of simple statistical measures to get a hold of it. Now, you will impute the missing values using **Mean Imputation** which is essentially imputing the mean of the respective column in place of missing values.\n",
    "\n",
    "__NOTE__: We do use **Mean Imputation** a lot; but it is neither the **best** nor the **only** imputation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Call the fillna() function with the imputation strategy\n",
    "data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "# Count the number of NaN values in each column to verify\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the famous [wine quality dataset](http://archive.ics.uci.edu/ml/datasets/Wine+Quality) to illustrate variable scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv ' , sep = ';')\n",
    "X = df.drop('quality' , 1).values # drop target variable\n",
    "y1 = df['quality'].values\n",
    "pd.DataFrame.hist(df, figsize = [15,15]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us build a model without scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = pd.Series(y1 <= 5)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder =LabelEncoder()\n",
    "label_encoder = label_encoder.fit(y)\n",
    "y = label_encoder.transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# is the rating <= 5?\n",
    "# plot histograms of original target variable\n",
    "# and aggregated target variable\n",
    "plt.figure(figsize=(20,5));\n",
    "plt.subplot(1, 2, 1 );\n",
    "plt.hist(y1);\n",
    "plt.xlabel('original target value')\n",
    "plt.ylabel('count')\n",
    "plt.subplot(1, 2, 2);\n",
    "plt.hist(y)\n",
    "plt.xlabel('aggregated target value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2019)\n",
    "\n",
    "from sklearn import neighbors, linear_model\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 5)\n",
    "knn_model_1 = knn.fit(X_train, y_train)\n",
    "print('k-NN accuracy for test set: %f' % knn_model_1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true, y_pred = y_test, knn_model_1.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running a model, such as regression (predicting a continuous variable) or classification (predicting a discrete variable), on data, you almost always want to do some preprocessing. For numerical variables, it is common to either normalize or standardize your data. What do these terms mean?\n",
    "\n",
    "All **normalization**means is scaling a dataset so that its minimum is `0` and its maximum `1`. To achieve this we transform each data point `x` to\n",
    "\n",
    "$$ x_{normalized} = \\frac{x - x_{min}}{x_{max} - x_{min}} $$\n",
    "\n",
    "__Stardardization__ is slightly different; it's job is to center the data around 0 and to scale with respect to the standard deviation:\n",
    "\n",
    "$$ x_{standardized} = \\frac{x-\\mu}{\\sigma} $$\n",
    "\n",
    "where $\\mu$ and $\\sigma$ are the mean and standard deviation of the dataset, respectively. First note that these transformations merely change the range of the data and not the distribution. You may later wish to use any other number of transforms, such as a log transform or a Box-Cox transform, to make your data look more Gaussian (like a bell-curve). But before we go further, it is important to ask the following questions: why do we scale our data? Are there times that it is more appropriate than others? For example, is it more important in classification problems than in regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "Xs = scale(X)\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xs_train, Xs_test, y_train, y_test = train_test_split(Xs, y, test_size=0.2, random_state=2019)\n",
    "knn_model_2 = knn.fit(Xs_train, y_train)\n",
    "print('k-NN score for test set: %f' % knn_model_2.score(Xs_test, y_test))\n",
    "#print('k-NN score for training set: %f' % knn_model_2.score(Xs_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = y_test, knn_model_2.predict(Xs_test)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these measures improved by `0.1`, which is a `10%` improvement and significant! As hinted at above, before scaling there were a number of predictor variables with ranges of different order of magnitudes, meaning that one or two of them could dominate in the context of an algorithm such as k-NN. The two main reasons for scaling your data are\n",
    "\n",
    "1. Your predictor variables may have significantly different ranges and, in certain situations, such as when implementing k-NN, this needs to be mitigated so that certain features do not dominate the algorithm;\n",
    "2. You want your features to be unit-independent, that is, not reliant on the scale of the measurement involved: for example, you could have a measured feature expressed in meters and I could have the same feature expressed in centimeters. If we both scale our respective data, this feature will be the same for each of us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will do variable encoding.\n",
    "\n",
    "Before that, you need a dataset which actually contains non-numeric data. You will use the famous [Iris dataset](http://archive.ics.uci.edu/ml/datasets/Iris) for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset to a DataFrame object iris\n",
    "iris = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\",header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See first 20 rows of the dataset\n",
    "iris.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily convert the string values to integer values using the [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html). The three class values (Iris-setosa, Iris-versicolor, Iris-virginica) are mapped to the integer values (`0, 1, 2`).\n",
    "\n",
    "In this case, the fourth column/feature of the dataset contains non-numeric values. So you need to separate it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame to a NumPy array\n",
    "iris = iris.values\n",
    "\n",
    "# Separate\n",
    "Y = iris[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encode string class values as integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(Y)\n",
    "label_encoded_y = label_encoder.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoded_y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoded_y[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics for model evaluation:\n",
    "You have designed and developed your machine learning model. Now, you want to evaluate the performance of your model on the test data. In this regards, you seek help of various statistical metrics like *Precision, Recall, ROC, AUC, RMSE*, etc. You also seek help from multiple data resampling techniques such as **k-fold Cross-Validation**.\n",
    "\n",
    "Statistics can effectively be used to:\n",
    "\n",
    "- [Estimate a hypothesis accuracy](https://machinelearningmastery.com/statistical-significance-tests-for-comparing-machine-learning-algorithms/)\n",
    "\n",
    "- [Determine the error of two hypotheses](https://www.universalclass.com/articles/math/statistics/types-of-errors-in-hypothesis-testing.htm)\n",
    "\n",
    "- [Compare learning algorithms using McNemar's test](https://machinelearningmastery.com/mcnemars-test-for-machine-learning/)\n",
    "\n",
    "It is important to note that the hypothesis refers to learned models; the results of running a learning algorithm on a dataset. Evaluating and comparing the hypothesis means comparing learned models, which is different from evaluating and comparing machine learning algorithms, which could be trained on different samples from the same problem or various problems.\n",
    "\n",
    "Let's study Gaussian and Descriptive statistics now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Gaussian and Descriptive stats:\n",
    "A sample of data is nothing but a snapshot from a broader population of all the potential observations that could be taken from a domain or generated by a process.\n",
    "\n",
    "Interestingly, many observations fit a typical pattern or distribution called the **normal distribution**, or more formally, the **Gaussian distribution**. This is the bell-shaped distribution that you may be aware of. The following figure denotes a Gaussian distribution:\n",
    "\n",
    "<img src = 'http://hyperphysics.phy-astr.gsu.edu/hbase/Math/immath/gauds.gif' />\n",
    "*Source: HyperPhysics*\n",
    "\n",
    "Gaussian processes and Gaussian distributions are whole another sub-fields unto themselves. But, you will now study two of the most essential ingredients that build the entire world of Gaussian distributions in general.\n",
    "\n",
    "Any sample data taken from a Gaussian distribution can be summarized with two parameters:\n",
    "\n",
    "- Mean: The central tendency or most likely value in the distribution (the top of the bell).\n",
    "- Variance: The average difference that observations have from the mean value in the distribution (the spread).\n",
    "\n",
    "The term variance also gives rise to another critical term, i.e., standard deviation, which is merely the square root of the variance.\n",
    "\n",
    "The mean, variance, and standard deviation can be directly calculated from data samples using numpy.\n",
    "\n",
    "You will first generate a sample of `100` random numbers pulled from a Gaussian distribution with a mean of `50` and a standard deviation of `5`. You will then calculate the summary statistics.\n",
    "\n",
    "First, you will import all the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Dependencies\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from numpy import mean\n",
    "from numpy import var\n",
    "from numpy import std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you set the random number generator seed so that your results are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate univariate observations\n",
    "numbers = 5 * randn(10000) + 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "print('Mean: %.3f' % mean(numbers))\n",
    "print('Variance: %.3f' % var(numbers))\n",
    "print('Standard Deviation: %.3f' % std(numbers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable correlation:\n",
    "Generally, the features that are contained in a dataset can often be related to each other which is very obvious to happen in practice. In statistical terms, this relationship between the features of your dataset (be it simple or complex) is often termed as **correlation**.\n",
    "\n",
    "It is crucial to find out the degree of the correlation of the features in a dataset. This step essentially serves you as feature selection which concerns selecting the **most important** features from a dataset. This step is one of the most vital steps in a standard machine learning pipeline as it can give you a tremendous accuracy boost that too within a lesser amount of time.\n",
    "\n",
    "__NOTE#1__: __Most important__ features usually refer to features with higher correlation with the target.\n",
    "\n",
    "__NOTE#2__: In linear models (e.g. linear regression), you do not want high correlation between your __features__.\n",
    "\n",
    "For better understanding and to keep it more practical let's understand why features can be related to each other:\n",
    "\n",
    "- One feature can be a determinant of another feature\n",
    "- One feature could be associated with another feature in some degree of composition\n",
    "- Multiple features can combine and give birth to another feature\n",
    "\n",
    "Correlation between the features can be of three types: - **Positive** correlation where both the feature change in the same direction, **Neutral** correlation when there is no relationship of the change in the two features, **Negative** correlation where both the features change in opposite directions.\n",
    "\n",
    "Correlation measurements form the fundamental of filter-based feature selection techniques. You can mathematically the relationship between samples of two variables using a statistical method called [Pearson’s correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient), named after the developer of the method, Karl Pearson.\n",
    "\n",
    "You can calculate the Pearson's correlation score by using the `corr()` function of `pandas` with the `method` parameter as `pearson`. Let's study the correlation between the features of the Pima Indians Diabetes dataset that you used earlier. You already have the data in good shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the matrix of correlation score between the features and the label\n",
    "scoreTable = data.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visulaize the matrix\n",
    "data.corr(method='pearson').style.format(\"{:.2}\").background_gradient(cmap=plt.get_cmap('coolwarm'), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see the Pearson's correlation between all the features and the label of the dataset.\n",
    "\n",
    "In the next section, you will study non-parametric statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-parametric statistics:\n",
    "A large portion of the field of statistics and statistical methods is dedicated to data where the distribution is known.\n",
    "\n",
    "Non-parametric statistics comes in handy when there is **no** or **few** information available about the population parameters. Non-parametric tests make no assumptions about the distribution of data.\n",
    "\n",
    "In the case where you are working with nonparametric data, specialized nonparametric statistical methods can be used that discard all information about the distribution. As such, these methods are often referred to as **distribution-free** methods.\n",
    "\n",
    "Bu before a nonparametric statistical method can be applied, the data must be converted into a rank format. Statistical methods that expect data in a rank format are sometimes called rank statistics. Examples of rank statistics can be rank correlation and rank statistical hypothesis tests. Ranking data is exactly as its name suggests.\n",
    "\n",
    "A widely used nonparametric statistical hypothesis test for checking for a difference between two independent samples is the Mann-Whitney U test, named for Henry Mann and Donald Whitney.\n",
    "\n",
    "You will implement this test in Python via the `mannwhitneyu()` which is provided by `SciPy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dependencies that you need\n",
    "from scipy.stats import mannwhitneyu\n",
    "from numpy.random import rand\n",
    "\n",
    "# seed the random number generator\n",
    "seed(2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate two independent samples\n",
    "data1 = 50 + (rand(100) * 10)\n",
    "data2 = 51 + (rand(100) * 10)\n",
    "# Compare samples\n",
    "stat, p = mannwhitneyu(data1, data2)\n",
    "print('Statistics = %.3f, p = %.3f' % (stat, p))\n",
    "# Interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    print('Same distribution (fail to reject H0)')\n",
    "else:\n",
    "    print('Different distribution (reject H0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`alpha` is the threshold parameter which is decided by you. The `mannwhitneyu()` returns two things:\n",
    "\n",
    "- statistic: The `Mann-Whitney U` statistic, equal to min(U for x, U for y) if alternative is equal to None (deprecated; exists for backward compatibility), and U for y otherwise.\n",
    "\n",
    "- pvalue: p-value assuming an asymptotic normal distribution.\n",
    "\n",
    "If you want to study the other methods of Non-parametric statistics, you can do it from here.\n",
    "\n",
    "The other two popular non-parametric statistical significance tests that you can use are:\n",
    "\n",
    "- [Friedman test](https://en.wikipedia.org/wiki/Friedman_test)\n",
    "- [Wilcoxon signed-rank test](https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test)\n",
    "\n",
    "\n",
    "## Wrap Up\n",
    "\n",
    "You have finally made it to the end. In this article, you studied a variety of essential statistical concepts that play very crucial role in your machine learning projects. So, understanding them is just important.\n",
    "\n",
    "From mere an introduction to statistics, you took it to statistical rankings that too with several implementations. That is definitely quite a feat. You studied three different datasets, exploited `pandas` and `numpy` functionalities to the fullest and moreover, you used `SciPy `as well. Next are some links for you if you want to take things further:\n",
    "\n",
    "- [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)\n",
    "- [Machine Learning book by Tom Mitchell](https://www.cs.ubbcluj.ro/~gabis/ml/ml-books/McGrawHill%20-%20Machine%20Learning%20-Tom%20Mitchell.pdf)\n",
    "- [All for Statistics](https://www.ic.unicamp.br/~wainer/cursos/1s2013/ml/livro.pdf)\n",
    "\n",
    "## References\n",
    "Following are the resources I took help from for writing this blog:\n",
    "\n",
    "- [Machine Learning Mastery mini course on Statistics](https://machinelearningmastery.com/statistics-for-machine-learning-mini-course/)\n",
    "- [A Gentle Introduction to Statistical Sampling and Resampling](https://machinelearningmastery.com/statistical-sampling-and-resampling/)\n",
    "- [Statistical Probability](https://www.khanacademy.org/math/statistics-probability)\n",
    "- [Statistical Learning course by Stanford University](https://statlearning.class.stanford.edu/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
